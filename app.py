import os
import streamlit as st
import fitz  # PyMuPDF
import pytesseract
from pdf2image import convert_from_bytes
from PIL import Image
import io
import pandas as pd
import json
import copy
import numpy as np

# –ò–º–ø–æ—Ä—Ç –∏–∑ –≤–∞—à–µ–≥–æ —Ñ–∞–π–ª–∞ —à–∞–±–ª–æ–Ω–æ–≤
from templates import REPORT_TEMPLATES, get_report_template_as_string, get_translation_map, get_report_codes

# –ò–º–ø–æ—Ä—Ç—ã LangChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.exceptions import OutputParserException

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
st.set_page_config(layout="wide", page_title="–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ—Ç—á–µ—Ç–∞")

# --- –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø LLM ---
try:
    # –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è –∫–ª—é—á–∞
    PROVIDER_API_KEY = st.secrets.get("NOVITA_API_KEY") or os.getenv("PROVIDER_API_KEY")
    if not PROVIDER_API_KEY:
        st.error("–ö–ª—é—á API –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è PROVIDER_API_KEY –∏–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ .streamlit/secrets.toml.")
        st.stop()
except Exception:
    st.error("–ö–ª—é—á API –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è PROVIDER_API_KEY –∏–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ .streamlit/secrets.toml.")
    st.stop()


# –í–∞—à–∞ –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á (–±—ã—Å—Ç—Ä–∞—è –∏ –Ω–µ–¥–æ—Ä–æ–≥–∞—è)
llm_main = ChatOpenAI(
    model_name="google/gemma-3-27b-it", # –í–∞—à–∞ —Ç–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å
    openai_api_key=PROVIDER_API_KEY,
    openai_api_base="https://api.novita.ai/v3/openai",
    temperature=0.1
)

# –ù–û–í–ê–Ø –ß–ê–°–¢–¨: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—â–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –Ω–æ –≤—Å–µ –µ—â–µ –Ω–∏–∑–∫—É—é –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏.
# –ú–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å 0.1, –µ—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Ç–∞–∫ —Ö–æ—Ä–æ—à–∏–µ.
llm_standardizer = ChatOpenAI(
    model_name="mistralai/mistral-nemo",
    openai_api_key=PROVIDER_API_KEY,
    openai_api_base="https://api.novita.ai/v3/openai",
    temperature=0.4 # –ú–æ–∂–Ω–æ –Ω–µ–º–Ω–æ–≥–æ –ø–æ–¥–Ω—è—Ç—å –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á
)

# --- –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø –§–£–ù–ö–¶–ò–ô ---

@st.cache_data
def extract_text_from_file(file_bytes, filename):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ (PDF –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º OCR –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
    try:
        ext = os.path.splitext(filename)[1].lower()
        text = ""
        
        if ext == ".pdf":
            # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é –∏–∑ PDF
            with fitz.open(stream=io.BytesIO(file_bytes), filetype="pdf") as doc:
                text = "".join(page.get_text() for page in doc)
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º OCR
            if len(text.strip()) < 150:
                st.warning(f"–¢–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π –≤ '{filename}' –ø—É—Å—Ç. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OCR...")
                images = convert_from_bytes(file_bytes, dpi=300)
                text = "\n".join([pytesseract.image_to_string(img, lang='rus+eng', config='--psm 6') for img in images])
        
        elif ext in [".png", ".jpg", ".jpeg"]:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é OCR
            image = Image.open(io.BytesIO(file_bytes))
            if image.mode != 'RGB':
                image = image.convert('RGB')
            text = pytesseract.image_to_string(image, lang='rus+eng', config='--psm 6')
        else:
            st.error(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {ext}")
            return None
            
        return text.strip()
    
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ '{filename}': {e}")
        return None

@st.cache_data
def classify_report(_llm, text: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM"""
    parser = StrOutputParser()
    prompt = ChatPromptTemplate.from_template(
        "–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞. –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –æ–¥–Ω–∏–º –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤: {report_types}.\n\n–¢–µ–∫—Å—Ç:\n---\n{text_snippet}\n---"
    )
    chain = prompt | _llm | parser
    report_types_str = ", ".join(REPORT_TEMPLATES.keys())
    response = chain.invoke({"text_snippet": text[:4000], "report_types": report_types_str})
    
    # –ü–æ–∏—Å–∫ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–∏–ø–∞ –æ—Ç—á–µ—Ç–∞ –≤ –æ—Ç–≤–µ—Ç–µ LLM
    for report_type in REPORT_TEMPLATES.keys():
        if report_type in response:
            return report_type
    return "Unknown"

@st.cache_data
def extract_raw_financial_data(_llm, text: str) -> list:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –≤ —Å—ã—Ä–æ–º –≤–∏–¥–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ—Ç—á–µ—Ç–∞"""
    parser = JsonOutputParser()
    prompt_text = (
        "–¢—ã ‚Äî —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ò–∑–≤–ª–µ–∫–∏ –í–°–ï —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ—Ç—á–µ—Ç–∞. "
        "–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON-–º–∞—Å—Å–∏–≤–æ–º –æ–±—ä–µ–∫—Ç–æ–≤. –ö–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å:\n"
        "- source_item: –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ –Ω–∞ —è–∑—ã–∫–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞\n"
        "- unit: –µ–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)\n"
        "- values: –º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏ period (–≥–æ–¥) –∏ value (—á–∏—Å–ª–æ)\n\n"
        "–ü–†–ê–í–ò–õ–ê:\n"
        "1. –í–∫–ª—é—á–∞–π –í–°–ï —Å—Ç–∞—Ç—å–∏ —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏\n"
        "2. –î–ª—è —Å—Ç–∞—Ç–µ–π —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–µ—Ä–∏–æ–¥–∞–º–∏ —Å–æ–∑–¥–∞–π –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç —Å –º–∞—Å—Å–∏–≤–æ–º values\n"
        "3. –ü–µ—Ä–∏–æ–¥ —É–∫–∞–∑—ã–≤–∞–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'YYYY'\n"
        "4. –ï—Å–ª–∏ –ø–µ—Ä–∏–æ–¥ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–π 'N/A'\n"
        "5. –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ —á–∏—Å–ª–æ–≤–æ–µ, –ø—Ä–æ–ø—É—Å–∫–∞–π –µ–≥–æ\n"
        "6. –ù–µ –ø—ã—Ç–∞–π—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç—å–∏!\n"
        "7. –°–æ—Ö—Ä–∞–Ω—è–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–∞—Ç–µ–π\n\n"
        "–ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞ –¥–ª—è –æ—Ç—á–µ—Ç–∞ —Å –¥–≤—É–º—è –ø–µ—Ä–∏–æ–¥–∞–º–∏:\n"
        "["
        "  {{\"source_item\": \"–í—ã—Ä—É—á–∫–∞\", \"unit\": \"—Ç—ã—Å. —Ä—É–±.\", \"values\": [ {{\"period\": \"2024\", \"value\": 150000}}, {{\"period\": \"2023\", \"value\": 120000}} ]}},"
        "  {{\"source_item\": \"–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–æ–¥–∞–∂\", \"unit\": \"—Ç—ã—Å. —Ä—É–±.\", \"values\": [ {{\"period\": \"2024\", \"value\": 90000}}, {{\"period\": \"2023\", \"value\": 75000}} ]}}"
        "]"
    )
    prompt = ChatPromptTemplate.from_messages([
        ("system", prompt_text),
        ("user", "–¢–µ–∫—Å—Ç –æ—Ç—á–µ—Ç–∞:\n---\n{text}\n---")
    ])
    chain = prompt | _llm | parser
    
    try:
        result = chain.invoke({"text": text[:100000]})
        return result
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
        return []

@st.cache_data
def correct_source_item_names(_llm, raw_data: list, report_type: str) -> list:
    """
    –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–∞—Ç–µ–π, –∏—Å–ø—Ä–∞–≤–ª—è—è –æ—à–∏–±–∫–∏ OCR –∏ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
    —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —à–∞–±–ª–æ–Ω–∞ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞
    """
    if not raw_data:
        return []

    # –ü–æ–ª—É—á–∞–µ–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–∞—Ç–µ–π –∏–∑ —à–∞–±–ª–æ–Ω–∞
    template_items = get_report_template_as_string(report_type)
    source_items_to_correct = list(set([item['source_item'] for item in raw_data]))

    parser = JsonOutputParser()
    prompt_text = (
        "–¢—ã ‚Äî –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –±—É—Ö–≥–∞–ª—Ç–µ—Ä –∏ –∫–æ—Ä—Ä–µ–∫—Ç–æ—Ä, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –æ—à–∏–±–æ–∫ OCR –≤ —Ä—É—Å—Å–∫–∏—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. "
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∏—Å–ø—Ä–∞–≤–∏—Ç—å –æ–ø–µ—á–∞—Ç–∫–∏, –æ—à–∏–±–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏ —Å—Ç—Ä–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º —Å–ø–∏—Å–∫–µ –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π. "
        "–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON-–æ–±—ä–µ–∫—Ç–æ–º, –≥–¥–µ –∫–ª—é—á ‚Äî —ç—Ç–æ –∏—Å—Ö–æ–¥–Ω–∞—è –æ—à–∏–±–æ—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞, –∞ –∑–Ω–∞—á–µ–Ω–∏–µ ‚Äî —ç—Ç–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä–æ–∫–∞.\n\n"
        "–ü–†–ê–í–ò–õ–ê:\n"
        "1. –°–æ—Ö—Ä–∞–Ω—è–π —Å–º—ã—Å–ª. 'neGuropoxaa anomxenocr' –¥–æ–ª–∂–Ω–æ —Å—Ç–∞—Ç—å '–î–µ–±–∏—Ç–æ—Ä—Å–∫–∞—è –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç—å', –∞ –Ω–µ '–ö—Ä–µ–¥–∏—Ç–æ—Ä—Å–∫–∞—è –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç—å'.\n"
        "2. –ò—Å–ø—Ä–∞–≤–ª—è–π —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–µ –æ—à–∏–±–∫–∏. –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ, –≤–µ—Ä–Ω–∏ –µ–≥–æ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.\n"
        "3. –ü—Ä–∏–≤–æ–¥–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –≤–∏–¥—É —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã.\n"
        "4. –í –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {template_items}\n\n"
        "–ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞:\n"
        '{{\n'
        '  "neGuropoxaa anomxenocr": "–î–µ–±–∏—Ç–æ—Ä—Å–∫–∞—è –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç—å",\n'
        '  "–ù–æ–º–∞—Ç–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∞–∫—Ç–∏–≤—ã": "–ù–µ–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–µ –∞–∫—Ç–∏–≤—ã",\n'
        '  "–í—ã—Ä—É—á–∫–∞": "–í—ã—Ä—É—á–∫–∞"\n'
        '}}\n\n'
        "–°–ø–∏—Å–æ–∫ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:\n{items_to_correct}"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", prompt_text),
        ("user", "–ò—Å–ø—Ä–∞–≤—å —ç—Ç–æ—Ç —Å–ø–∏—Å–æ–∫ –∏ –≤–µ—Ä–Ω–∏ JSON-–æ–±—ä–µ–∫—Ç. –í–∫–ª—é—á–∏ –≤ –æ—Ç–≤–µ—Ç –í–°–ï –∫–ª—é—á–∏ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞.")
    ])

    chain = prompt | _llm | parser

    try:
        items_json_string = json.dumps(source_items_to_correct, ensure_ascii=False)
        correction_map = chain.invoke({
            "template_items": template_items,
            "items_to_correct": items_json_string
        })

        corrected_raw_data = copy.deepcopy(raw_data) # –ì–ª—É–±–æ–∫–∞—è –∫–æ–ø–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

        for item in corrected_raw_data:
            original_item = item['source_item']
            if original_item in correction_map:
                item['source_item'] = correction_map[original_item]

        return corrected_raw_data
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π: {e}")
        return raw_data # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏

@st.cache_data
def standardize_data(_llm, raw_data: list, report_type: str) -> dict:
    """–°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º —à–∞–±–ª–æ–Ω–æ–º –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏"""
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–¥—ã —Å—Ç–∞—Ç–µ–π –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ –æ—Ç—á–µ—Ç–∞
    report_codes = get_report_codes(report_type)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É —Å –∫–æ–¥–∞–º–∏ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
    codes_str = "\n".join([f"{item}: {code}" for item, code in report_codes.items()])
    
    parser = JsonOutputParser()
    prompt_text = (
        "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏. –°–æ–ø–æ—Å—Ç–∞–≤—å —Å—ã—Ä—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º —à–∞–±–ª–æ–Ω–æ–º. "
        "–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON-–æ–±—ä–µ–∫—Ç–æ–º —Å –î–í–£–ú–Ø –∫–ª—é—á–∞–º–∏: `standardized_data` –∏ `unmapped_items`.\n\n"
        "1. –ö–ª—é—á `standardized_data` –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ú–ê–°–°–ò–í –æ–±—ä–µ–∫—Ç–æ–≤, —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å —à–∞–±–ª–æ–Ω–æ–º, –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n"
        "   {{\n"
        "     \"line_item\": \"–Ω–∞–∑–≤–∞–Ω–∏–µ_–∏–∑_—à–∞–±–ª–æ–Ω–∞\",\n"
        "     \"code\": \"–∫–æ–¥_—Å—Ç–∞—Ç—å–∏\",\n"
        "     \"unit\": \"–µ–¥_–∏–∑–º\",\n"
        "     \"values_by_period\": [\n"
        "         {{\"period\": \"2024\", \"value\": —á–∏—Å–ª–æ, \"components\": [{{\"source_item\": \"–∏—Å—Ö–æ–¥–Ω–∞—è_—Å—Ç–∞—Ç—å—è\", \"source_value\": —á–∏—Å–ª–æ}}]}}\n"
        "     ]\n"
        "   }}\n\n"
        "2. –ö–ª—é—á `unmapped_items` –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ú–ê–°–°–ò–í –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï —É–¥–∞–ª–æ—Å—å —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–∏ —Å –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–µ–π —à–∞–±–ª–æ–Ω–∞. –°–æ—Ö—Ä–∞–Ω–∏ –∏—Ö –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç.\n\n"
        "–ü–†–ê–í–ò–õ–ê –°–û–ü–û–°–¢–ê–í–õ–ï–ù–ò–Ø:\n"
        "- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Å—Ç–∞—Ç—å–∏ –∏–∑ —ç—Ç–æ–≥–æ —à–∞–±–ª–æ–Ω–∞: {template_items}\n"
        "- –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–∏ —É–∫–∞–∂–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥: {codes}\n"
        "- –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏ –Ω–∞–π–¥–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—ã—Ä—ã–µ —Å—Ç–∞—Ç—å–∏ –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–π (—Å—É–º–º–∏—Ä—É–π) –∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è.\n"
        "- –ï—Å–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å—Ç–∞—Ç—å—è —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—ã—Ä—ã—Ö, —É–∫–∞–∂–∏ –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã.\n"
        "- –ï—Å–ª–∏ —Å—ã—Ä–∞—è —Å—Ç–∞—Ç—å—è –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É, –ø–æ–º–µ—Å—Ç–∏ –µ–µ –≤ `unmapped_items`.\n"
        "- –ï—Å–ª–∏ –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –Ω–µ –≤–∫–ª—é—á–∞–π –µ–µ –≤ `standardized_data`.\n\n"
        "–°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ:\n{raw_data}"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", prompt_text),
        ("user", "–í—ã–ø–æ–ª–Ω–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–≥–æ –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º –∏ –≤–µ—Ä–Ω–∏ JSON —Å –∫–ª—é—á–∞–º–∏ 'standardized_data' –∏ 'unmapped_items'.")
    ])

    chain = prompt | _llm | parser
    try:
        raw_data_str = json.dumps(raw_data, ensure_ascii=False, indent=2)[:100000]
        template_items = get_report_template_as_string(report_type)
        result = chain.invoke({
            "template_items": template_items,
            "codes": codes_str,
            "raw_data": raw_data_str
        })
        
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –µ—Å—Ç—å –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏
        if "standardized_data" not in result:
            result["standardized_data"] = []
        if "unmapped_items" not in result:
            result["unmapped_items"] = []
            
        return result
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏: {e}")
        return {"standardized_data": [], "unmapped_items": raw_data}

def flatten_data_for_display(data: list, report_type: str) -> list:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –ø–ª–æ—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    flat_list = []
    # –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –ø–µ—Ä–µ–≤–æ–¥–∞ (—Ç–µ–ø–µ—Ä—å —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤–∞—Ä–µ–π)
    translation_dict = get_translation_map(report_type)
    
    for item in data:
        english_name = item.get("line_item")
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–∞—Ç—å–µ: —Å–ª–æ–≤–∞—Ä—å —Å 'ru' –∏ 'code'
        item_info = translation_dict.get(english_name, {})
        russian_name = item_info.get("ru", english_name)  # –ï—Å–ª–∏ –Ω–µ—Ç, –æ—Å—Ç–∞–≤–ª—è–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–æ–µ
        code = item_info.get("code", "")  # –ö–æ–¥ —Å—Ç–∞—Ç—å–∏
        
        values_by_period = item.get("values_by_period", [])
        if not values_by_period: 
            continue
            
        for period_data in values_by_period:
            value = period_data.get("value")
            if value is None: 
                continue
                
            flat_list.append({
                "–ö–æ–¥": code,
                "–°—Ç–∞—Ç—å—è (RU)": russian_name, 
                "Line Item (EN)": english_name, 
                "unit": item.get("unit"),
                "period": period_data.get("period"), 
                "value": value,
                "components": period_data.get("components", [])
            })
    return flat_list

def display_raw_data(raw_data):
    """–°–æ–∑–¥–∞–µ—Ç DataFrame –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    if not raw_data:
        return pd.DataFrame()
    rows = []
    for item in raw_data:
        for val in item.get("values", []):
            rows.append({
                "–ò—Å—Ö–æ–¥–Ω–∞—è —Å—Ç–∞—Ç—å—è": item.get('source_item', 'N/A'),
                "–ü–µ—Ä–∏–æ–¥": val.get("period", "N/A"),
                "–ó–Ω–∞—á–µ–Ω–∏–µ": val.get("value", "N/A"),
                "–ï–¥. –∏–∑–º.": item.get("unit", "")
            })
    return pd.DataFrame(rows)

def to_excel_bytes(wide_df, long_df):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç DataFrame –≤ –±–∞–π—Ç—ã Excel —Ñ–∞–π–ª–∞ —Å –¥–≤—É–º—è –ª–∏—Å—Ç–∞–º–∏"""
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        wide_df.to_excel(writer, index=False, sheet_name='–û—Ç—á–µ—Ç')
        long_df.to_excel(writer, index=False, sheet_name='–î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è')
    return output.getvalue()

def transform_to_wide_format(long_df):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –≤ —à–∏—Ä–æ–∫–∏–π —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞"""
    # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    wide_df = long_df.pivot_table(
        index=['–ö–æ–¥ —Å—Ç–∞—Ç—å–∏', '–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è', '–ï–¥. –∏–∑–º.'],
        columns='–ü–µ—Ä–∏–æ–¥',
        values='–ò—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ',
        aggfunc='first'  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ)
    ).reset_index()
    
    # –ü–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –ø–µ—Ä–∏–æ–¥–∞–º–∏
    wide_df.columns.name = None  # –£–±–∏—Ä–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –∫–æ–ª–æ–Ω–æ–∫
    
    # –ü–µ—Ä–µ—Å—Ç–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏: –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–µ—Ä–∏–æ–¥ –ø–µ—Ä–≤—ã–º
    period_cols = [col for col in wide_df.columns if col not in ['–ö–æ–¥ —Å—Ç–∞—Ç—å–∏', '–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è', '–ï–¥. –∏–∑–º.']]
    period_cols.sort(reverse=True)  # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–µ—Ä–∏–æ–¥—ã –ø–æ —É–±—ã–≤–∞–Ω–∏—é (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
    wide_df = wide_df[['–ö–æ–¥ —Å—Ç–∞—Ç—å–∏', '–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è', '–ï–¥. –∏–∑–º.'] + period_cols]
    
    return wide_df


# --- –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ò–ù–¢–ï–†–§–ï–ô–° –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø ---
st.title("üìä –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ—Ç—á–µ—Ç–∞")

st.sidebar.header("–ó–∞–≥—Ä—É–∑–∫–∞ –§–∞–π–ª–æ–≤")
uploaded_files = st.sidebar.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å–∫–∞–Ω—ã –æ—Ç—á–µ—Ç–∞", type=["pdf", "png", "jpg", "jpeg"], accept_multiple_files=True
)

if uploaded_files:
    file_names = [f.name for f in uploaded_files]
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    if "processed_data" not in st.session_state or st.session_state.get("file_names") != file_names:
        st.session_state.file_names = file_names
        st.session_state.all_text = ""
        st.session_state.raw_data = None
        st.session_state.corrected_raw_data = None
        st.session_state.processed_data = None
        st.session_state.unmapped_items = None

        with st.spinner("–®–∞–≥ 1/5: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–æ–≤..."):
            all_text = ""
            for uploaded_file in uploaded_files:
                file_text = extract_text_from_file(uploaded_file.getvalue(), uploaded_file.name)
                if file_text:
                    all_text += f"\n\n--- –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: {uploaded_file.name} ---\n\n{file_text}"
            st.session_state.all_text = all_text.strip()

    all_text = st.session_state.get("all_text", "")
    if not all_text:
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç.")
        st.stop()

    st.info(f"üìù –û–±—â–∏–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞: {len(all_text)} —Å–∏–º–≤–æ–ª–æ–≤.")

    # –®–∞–≥ 2: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    with st.spinner("üîç –®–∞–≥ 2/5: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –æ—Ç—á–µ—Ç–∞..."):
        report_type = classify_report(llm_main, all_text)

    if report_type == "Unknown":
        st.error("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –æ—Ç—á–µ—Ç–∞.")
        st.stop()
    else:
        st.success(f"‚úÖ –û—Ç—á–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ **{report_type}**.")

    # –®–∞–≥ 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if st.session_state.get("raw_data") is None:
        with st.spinner("üìã –®–∞–≥ 3/5: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö..."):
            raw_data = extract_raw_financial_data(llm_main, all_text)
            st.session_state.raw_data = raw_data

    if st.session_state.raw_data:
        st.success("‚úÖ –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã!")
        with st.expander("üîé –ü—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–µ —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ (–¥–æ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏)", expanded=False):
            raw_df = display_raw_data(st.session_state.raw_data)
            st.dataframe(raw_df, use_container_width=True, hide_index=True)

    # –®–∞–≥ 4: –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π
    if st.session_state.raw_data and st.session_state.get("corrected_raw_data") is None:
        with st.spinner("‚úçÔ∏è –®–∞–≥ 4/5: –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π..."):
            corrected_data = correct_source_item_names(llm_main, st.session_state.raw_data, report_type)
            st.session_state.corrected_raw_data = corrected_data
            
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ –∏ –ø–æ—Å–ª–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏
            with st.expander("üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ –∏ –ø–æ—Å–ª–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏", expanded=True):
                comparison_list = []
                for original, corrected in zip(st.session_state.raw_data, st.session_state.corrected_raw_data):
                    if original['source_item'] != corrected['source_item']:
                        comparison_list.append({
                            "–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ": original['source_item'],
                            "–°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ": corrected['source_item']
                        })
                if comparison_list:
                    comparison_df = pd.DataFrame(comparison_list).drop_duplicates().reset_index(drop=True)
                    st.dataframe(comparison_df, use_container_width=True, hide_index=True)
                else:
                    st.info("–ò–∑–º–µ–Ω–µ–Ω–∏–π –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å. –í—Å–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã.")

    # –®–∞–≥ 5: –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    if st.session_state.get("corrected_raw_data") and st.session_state.get("processed_data") is None:
        with st.spinner("üîÑ –®–∞–≥ 5/5: –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö..."):
            response_dict = standardize_data(llm_standardizer, st.session_state.corrected_raw_data, report_type)
            st.session_state.processed_data = response_dict.get("standardized_data", [])
            st.session_state.unmapped_items = response_dict.get("unmapped_items", [])

    # --- –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---
    if st.session_state.get("processed_data") is not None:
        st.success("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω—ã!")

        flat_data = flatten_data_for_display(st.session_state.processed_data, report_type)
        if flat_data:
            # –°–æ–∑–¥–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π DataFrame
            long_df = pd.DataFrame(flat_data)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            def format_components(components_list):
                if not components_list or not isinstance(components_list, list):
                    return "–ü—Ä—è–º–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ"
                return "; ".join([f"{c.get('source_item', 'N/A')} ({c.get('source_value', 'N/A')})" for c in components_list])

            long_df['–ò—Å—Ç–æ—á–Ω–∏–∫ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏'] = long_df['components'].apply(format_components)
            
            # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
            long_df.rename(columns={
                '–ö–æ–¥': '–ö–æ–¥ —Å—Ç–∞—Ç—å–∏',
                '–°—Ç–∞—Ç—å—è (RU)': '–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è',
                'value': '–ò—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ',
                'period': '–ü–µ—Ä–∏–æ–¥',
                'unit': '–ï–¥. –∏–∑–º.'
            }, inplace=True)
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–¥—É –∏ –ø–µ—Ä–∏–æ–¥—É (–ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø–µ—Ä–∏–æ–¥–∞)
            long_df.sort_values(by=['–ö–æ–¥ —Å—Ç–∞—Ç—å–∏', '–ü–µ—Ä–∏–æ–¥'], ascending=[True, False], inplace=True)
            
            # –°–æ–∑–¥–∞–µ–º —à–∏—Ä–æ–∫–∏–π —Ñ–æ—Ä–º–∞—Ç
            wide_df = transform_to_wide_format(long_df)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            detail_columns = ["–ö–æ–¥ —Å—Ç–∞—Ç—å–∏", "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è", "–ò—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ", 
                             "–ü–µ—Ä–∏–æ–¥", "–ò—Å—Ç–æ—á–Ω–∏–∫ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏", "–ï–¥. –∏–∑–º."]
            
            # --- –í–´–ë–û–† –§–û–†–ú–ê–¢–ê –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø ---
            display_format = st.radio("–§–æ—Ä–º–∞—Ç –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è:", 
                                     ["–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π (–ø–µ—Ä–∏–æ–¥—ã –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö)", "–î–µ—Ç–∞–ª—å–Ω—ã–π (—Å–æ —Å–ø–∏—Å–∫–æ–º –ø–µ—Ä–∏–æ–¥–æ–≤)"])
            
            if display_format == "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π (–ø–µ—Ä–∏–æ–¥—ã –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö)":
                st.dataframe(wide_df, use_container_width=True, hide_index=True)
            else:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                st.dataframe(long_df[detail_columns], use_container_width=True, hide_index=True)
            
            # --- –≠–ö–°–ü–û–†–¢ –í EXCEL ---
            excel_bytes = to_excel_bytes(wide_df, long_df[detail_columns])
            st.download_button(
                "üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á–µ—Ç –≤ Excel", 
                excel_bytes, 
                f"standard_report_{report_type.replace(' ', '_')}.xlsx",
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        else:
            st.warning("–ü–æ—Å–ª–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–ø—Ä–∏–Ω—è—Ç—ã—Ö —Å—Ç–∞—Ç–µ–π
        if st.session_state.get("unmapped_items"):
            st.warning("‚ö†Ô∏è –°–ª–µ–¥—É—é—â–∏–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –Ω–µ –±—ã–ª–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω—ã —Å —à–∞–±–ª–æ–Ω–æ–º:")
            unmapped_df = display_raw_data(st.session_state.unmapped_items)
            if not unmapped_df.empty:
                st.dataframe(unmapped_df, use_container_width=True, hide_index=True)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        with st.expander("üìÑ –ü–æ–∫–∞–∑–∞—Ç—å JSON —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"):
            st.json(st.session_state.processed_data)
        with st.expander("üìÑ –ü–æ–∫–∞–∑–∞—Ç—å JSON –Ω–µ–ø—Ä–∏–Ω—è—Ç—ã—Ö —Å—Ç–∞—Ç–µ–π"):
            st.json(st.session_state.unmapped_items)
        with st.expander("üìù –ü–æ–∫–∞–∑–∞—Ç—å –≤–µ—Å—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"):
            st.text_area("–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç", all_text, height=400)
else:
    st.info("üëà –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑.")
