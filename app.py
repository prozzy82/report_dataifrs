import os
import streamlit as st
import fitz  # PyMuPDF
import pytesseract
from pdf2image import convert_from_bytes
from PIL import Image
import io
import pandas as pd
import json
import copy
import numpy as np

# –ò–º–ø–æ—Ä—Ç —à–∞–±–ª–æ–Ω–æ–≤
from templates import REPORT_TEMPLATES, get_report_template_as_string, get_translation_map, get_report_codes

# –ò–º–ø–æ—Ä—Ç –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ –º–æ–¥—É–ª—è —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏
from ifrs_taxonomy_helper import load_ifrs_taxonomy, suggest_mapping_from_taxonomy, build_unmapped_with_suggestions

# LangChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.exceptions import OutputParserException

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
st.set_page_config(layout="wide", page_title="–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ—Ç—á–µ—Ç–∞")

# API –∫–ª—é—á
try:
    PROVIDER_API_KEY = st.secrets.get("NOVITA_API_KEY") or os.getenv("PROVIDER_API_KEY")
    if not PROVIDER_API_KEY:
        st.error("–ö–ª—é—á API –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PROVIDER_API_KEY –∏–ª–∏ secrets.toml.")
        st.stop()
except Exception:
    st.error("–ö–ª—é—á API –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    st.stop()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
llm = ChatOpenAI(
    model_name="google/gemma-3-27b-it",
    openai_api_key=PROVIDER_API_KEY,
    openai_api_base="https://api.novita.ai/v3/openai",
    temperature=0.1
)

# ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ IFRS —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏
IFRS_TAXONOMY = load_ifrs_taxonomy("json_taxonomy_rus.json")

# --- –í–ê–®–ò –§–£–ù–ö–¶–ò–ò (extract_text_from_file, classify_report, extract_raw_financial_data, correct_source_item_names,
# standardize_data, flatten_data_for_display, display_raw_data, to_excel_bytes, transform_to_wide_format) –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π ---
# (–ü—Ä–æ–ø—É—â–µ–Ω–æ –∑–¥–µ—Å—å —Ä–∞–¥–∏ –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏ ‚Äî —Ç—ã —É–∂–µ –≤–∫–ª—é—á–∏–ª –∏—Ö —Ä–∞–Ω–µ–µ –∏ –æ–Ω–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã.)

# --- –ò–ù–¢–ï–†–§–ï–ô–° ---

st.title("üìä –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ—Ç—á–µ—Ç–∞")

st.sidebar.header("–ó–∞–≥—Ä—É–∑–∫–∞ –§–∞–π–ª–æ–≤")
uploaded_files = st.sidebar.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å–∫–∞–Ω—ã –æ—Ç—á–µ—Ç–∞", type=["pdf", "png", "jpg", "jpeg"], accept_multiple_files=True)

if uploaded_files:
    file_names = [f.name for f in uploaded_files]
    if "processed_data" not in st.session_state or st.session_state.get("file_names") != file_names:
        st.session_state.file_names = file_names
        st.session_state.all_text = ""
        st.session_state.raw_data = None
        st.session_state.corrected_raw_data = None
        st.session_state.processed_data = None
        st.session_state.unmapped_items = None

        with st.spinner("–®–∞–≥ 1/5: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–æ–≤..."):
            all_text = ""
            for uploaded_file in uploaded_files:
                file_text = extract_text_from_file(uploaded_file.getvalue(), uploaded_file.name)
                if file_text:
                    all_text += f"\n\n--- –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: {uploaded_file.name} ---\n\n{file_text}"
            st.session_state.all_text = all_text.strip()

    all_text = st.session_state.get("all_text", "")
    if not all_text:
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç.")
        st.stop()

    st.info(f"üìù –û–±—â–∏–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞: {len(all_text)} —Å–∏–º–≤–æ–ª–æ–≤.")

    with st.spinner("üîç –®–∞–≥ 2/5: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –æ—Ç—á–µ—Ç–∞..."):
        report_type = classify_report(llm, all_text)

    if report_type == "Unknown":
        st.error("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –æ—Ç—á–µ—Ç–∞.")
        st.stop()
    else:
        st.success(f"‚úÖ –û—Ç—á–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ **{report_type}**.")

    if st.session_state.get("raw_data") is None:
        with st.spinner("üìã –®–∞–≥ 3/5: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö..."):
            raw_data = extract_raw_financial_data(llm, all_text)
            st.session_state.raw_data = raw_data

    if st.session_state.raw_data:
        st.success("‚úÖ –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã!")
        with st.expander("üîé –ü—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–µ —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ", expanded=False):
            raw_df = display_raw_data(st.session_state.raw_data)
            st.dataframe(raw_df, use_container_width=True, hide_index=True)

    if st.session_state.raw_data and st.session_state.get("corrected_raw_data") is None:
        with st.spinner("‚úçÔ∏è –®–∞–≥ 4/5: –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π..."):
            corrected_data = correct_source_item_names(llm, st.session_state.raw_data, report_type)
            st.session_state.corrected_raw_data = corrected_data

    if st.session_state.get("corrected_raw_data") and st.session_state.get("processed_data") is None:
        with st.spinner("üîÑ –®–∞–≥ 5/5: –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö..."):
            result = standardize_data(llm, st.session_state.corrected_raw_data, report_type)
            st.session_state.processed_data = result.get("standardized_data", [])
            st.session_state.unmapped_items = result.get("unmapped_items", [])

    if st.session_state.get("processed_data") is not None:
        flat_data = flatten_data_for_display(st.session_state.processed_data, report_type)
        if flat_data:
            long_df = pd.DataFrame(flat_data)
            long_df['–ò—Å—Ç–æ—á–Ω–∏–∫ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏'] = long_df['components'].apply(lambda comps: "; ".join(
                [f"{c.get('source_item')} ({c.get('source_value')})" for c in comps] if comps else ["–ü—Ä—è–º–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ"]))

            long_df.rename(columns={
                '–ö–æ–¥': '–ö–æ–¥ —Å—Ç–∞—Ç—å–∏',
                '–°—Ç–∞—Ç—å—è (RU)': '–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è',
                'value': '–ò—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ',
                'period': '–ü–µ—Ä–∏–æ–¥',
                'unit': '–ï–¥. –∏–∑–º.'
            }, inplace=True)

            long_df.sort_values(by=['–ö–æ–¥ —Å—Ç–∞—Ç—å–∏', '–ü–µ—Ä–∏–æ–¥'], ascending=[True, False], inplace=True)
            wide_df = transform_to_wide_format(long_df)

            display_format = st.radio("–§–æ—Ä–º–∞—Ç –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è:", ["–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π (–ø–µ—Ä–∏–æ–¥—ã –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö)", "–î–µ—Ç–∞–ª—å–Ω—ã–π"])
            if display_format == "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π (–ø–µ—Ä–∏–æ–¥—ã –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö)":
                st.dataframe(wide_df, use_container_width=True, hide_index=True)
            else:
                st.dataframe(long_df[["–ö–æ–¥ —Å—Ç–∞—Ç—å–∏", "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è", "–ò—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ", "–ü–µ—Ä–∏–æ–¥", "–ò—Å—Ç–æ—á–Ω–∏–∫ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏", "–ï–¥. –∏–∑–º."]], use_container_width=True)

            excel_bytes = to_excel_bytes(wide_df, long_df)
            st.download_button("üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á–µ—Ç –≤ Excel", excel_bytes, f"standard_report_{report_type.replace(' ', '_')}.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        else:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")

        if st.session_state.get("unmapped_items"):
            st.warning("‚ö†Ô∏è –ù–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏:")
            st.dataframe(display_raw_data(st.session_state.unmapped_items), use_container_width=True)

            # ‚úÖ –ü–û–î–°–ö–ê–ó–ö–ò –ù–ê –û–°–ù–û–í–ï –¢–ê–ö–°–û–ù–û–ú–ò–ò
            with st.expander("üí° –ü–æ–¥—Å–∫–∞–∑–∫–∏ –ø–æ –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º —Å—Ç–∞—Ç—å—è–º (—Ç–∞–∫—Å–æ–Ω–æ–º–∏—è IFRS)", expanded=False):
                for item in st.session_state.unmapped_items:
                    source = item.get("source_item", "")
                    suggestion = suggest_mapping_from_taxonomy(source, IFRS_TAXONOMY)
                    if suggestion:
                        st.markdown(f"üîé *{source}* –≤–æ–∑–º–æ–∂–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç: **{suggestion}**")
                    else:
                        st.markdown(f"‚ö†Ô∏è *{source}* ‚Äî —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

            # ‚úÖ –≠–ö–°–ü–û–†–¢ –ü–û–î–°–ö–ê–ó–û–ö
            with st.expander("üì• –°–∫–∞—á–∞—Ç—å –ø–æ–¥—Å–∫–∞–∑–∫–∏ –ø–æ –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º —Å—Ç–∞—Ç—å—è–º"):
                suggestion_df = pd.DataFrame(build_unmapped_with_suggestions(st.session_state.unmapped_items, IFRS_TAXONOMY))
                st.dataframe(suggestion_df, use_container_width=True, hide_index=True)

                excel_output = io.BytesIO()
                with pd.ExcelWriter(excel_output, engine='xlsxwriter') as writer:
                    suggestion_df.to_excel(writer, index=False, sheet_name="IFRS_Suggestions")

                file_name = f"unmapped_suggestions_{report_type.replace(' ', '_')}.xlsx"
                st.download_button("üíæ –°–∫–∞—á–∞—Ç—å Excel", data=excel_output.getvalue(), file_name=file_name)

        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        with st.expander("üìÑ JSON —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"):
            st.json(st.session_state.processed_data)
        with st.expander("üìÑ JSON –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"):
            st.json(st.session_state.unmapped_items)
        with st.expander("üìù –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"):
            st.text_area("–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç", all_text, height=400)
else:
    st.info("üëà –ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF/–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏")
