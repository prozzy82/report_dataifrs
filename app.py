import os
import streamlit as st
import fitz  # PyMuPDF
import pytesseract
from pdf2image import convert_from_bytes
from PIL import Image
import io
import pandas as pd
import json
import copy
import numpy as np

# --- –ù–û–í–´–ï –ò–ú–ü–û–†–¢–´ ---
# –ò–º–ø–æ—Ä—Ç –∏–∑ –≤–∞—à–µ–≥–æ –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏
from taxonomy import IFRS_TAXONOMY, IFRS_FLAT_LIST
# –ò–º–ø–æ—Ä—Ç –∏–∑ –≤–∞—à–µ–≥–æ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è —à–∞–±–ª–æ–Ω–æ–≤ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
from templates import get_translation_map, get_report_codes, REPORT_TEMPLATES

# –ò–º–ø–æ—Ä—Ç—ã LangChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
st.set_page_config(layout="wide", page_title="–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞")

# --- –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø LLM ---
try:
    PROVIDER_API_KEY = st.secrets.get("NOVITA_API_KEY") or os.getenv("PROVIDER_API_KEY")
    if not PROVIDER_API_KEY:
        st.error("–ö–ª—é—á API –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        st.stop()
except Exception:
    st.error("–ö–ª—é—á API –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    st.stop()

llm = ChatOpenAI(
    model_name="google/gemma-3-27b-it",
    openai_api_key=PROVIDER_API_KEY,
    openai_api_base="https://api.novita.ai/v3/openai",
    temperature=0.0
)

# --- –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø –§–£–ù–ö–¶–ò–ô ---

# –§—É–Ω–∫—Ü–∏–∏ extract_text_from_file, classify_report, extract_raw_financial_data, correct_source_item_names
# –æ—Å—Ç–∞—é—Ç—Å—è —Ç–∞–∫–∏–º–∏ –∂–µ, –∫–∞–∫ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–∏. –Ø –∏—Ö –æ—Å—Ç–∞–≤–ª—é –∑–¥–µ—Å—å –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã —Ñ–∞–π–ª–∞.

@st.cache_data
def extract_text_from_file(file_bytes, filename):
    try:
        ext = os.path.splitext(filename)[1].lower()
        text = ""
        if ext == ".pdf":
            with fitz.open(stream=io.BytesIO(file_bytes), filetype="pdf") as doc:
                text = "".join(page.get_text() for page in doc)
            if len(text.strip()) < 150:
                st.warning(f"–¢–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π –≤ '{filename}' –ø—É—Å—Ç. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OCR...")
                images = convert_from_bytes(file_bytes, dpi=300)
                text = "\n".join([pytesseract.image_to_string(img, lang='rus+eng', config='--psm 6') for img in images])
        elif ext in [".png", ".jpg", ".jpeg"]:
            image = Image.open(io.BytesIO(file_bytes))
            if image.mode != 'RGB': image = image.convert('RGB')
            text = pytesseract.image_to_string(image, lang='rus+eng', config='--psm 6')
        else:
            st.error(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {ext}"); return None
        return text.strip()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ '{filename}': {e}"); return None

@st.cache_data
def classify_report(_llm, text: str) -> str:
    parser = StrOutputParser()
    prompt = ChatPromptTemplate.from_template("–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞. –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –æ–¥–Ω–∏–º –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤: {report_types}.\n\n–¢–µ–∫—Å—Ç:\n---\n{text_snippet}\n---")
    chain = prompt | _llm | parser
    report_types_str = ", ".join(REPORT_TEMPLATES.keys())
    response = chain.invoke({"text_snippet": text[:4000], "report_types": report_types_str})
    for report_type in REPORT_TEMPLATES.keys():
        if report_type in response: return report_type
    return "Unknown"

@st.cache_data
def extract_raw_financial_data(_llm, text: str) -> list:
    parser = JsonOutputParser()
    prompt_text = (
        "–¢—ã ‚Äî —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ò–∑–≤–ª–µ–∫–∏ –í–°–ï —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ—Ç—á–µ—Ç–∞. "
        "–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON-–º–∞—Å—Å–∏–≤–æ–º –æ–±—ä–µ–∫—Ç–æ–≤. –ö–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å:\n"
        "- source_item: –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ –Ω–∞ —è–∑—ã–∫–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞\n"
        "- unit: –µ–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)\n"
        "- values: –º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏ period (–≥–æ–¥) –∏ value (—á–∏—Å–ª–æ)\n\n"
        "–ü–†–ê–í–ò–õ–ê:\n"
        "1. –í–∫–ª—é—á–∞–π –í–°–ï —Å—Ç–∞—Ç—å–∏ —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.\n2. –ü–µ—Ä–∏–æ–¥ —É–∫–∞–∑—ã–≤–∞–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'YYYY'.\n"
        "3. –°–æ—Ö—Ä–∞–Ω—è–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–∞—Ç–µ–π.\n\n"
        "–ü—Ä–∏–º–µ—Ä: [ {{\"source_item\": \"–í—ã—Ä—É—á–∫–∞\", \"unit\": \"—Ç—ã—Å. —Ä—É–±.\", \"values\": [ {{\"period\": \"2024\", \"value\": 150000}}, {{\"period\": \"2023\", \"value\": 120000}} ]}} ]"
    )
    prompt = ChatPromptTemplate.from_messages([("system", prompt_text), ("user", "–¢–µ–∫—Å—Ç –æ—Ç—á–µ—Ç–∞:\n---\n{text}\n---")])
    chain = prompt | _llm | parser
    try:
        return chain.invoke({"text": text[:100000]})
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}"); return []

@st.cache_data
def correct_source_item_names(_llm, raw_data: list) -> list:
    if not raw_data: return []
    source_items_to_correct = list(set([item['source_item'] for item in raw_data]))
    parser = JsonOutputParser()
    prompt_text = (
        "–¢—ã ‚Äî –∫–æ—Ä—Ä–µ–∫—Ç–æ—Ä, –∏—Å–ø—Ä–∞–≤–ª—è–µ—à—å –æ—à–∏–±–∫–∏ OCR –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π. –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON-–æ–±—ä–µ–∫—Ç–æ–º, –≥–¥–µ –∫–ª—é—á ‚Äî –∏—Å—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞, –∞ –∑–Ω–∞—á–µ–Ω–∏–µ ‚Äî –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è.\n\n"
        "–ü–†–ê–í–ò–õ–ê:\n"
        "1. –ò—Å–ø—Ä–∞–≤–ª—è–π —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–µ –æ—à–∏–±–∫–∏ OCR –∏ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏. 'Bupyuka' -> '–í—ã—Ä—É—á–∫–∞'.\n"
        "2. –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ, –≤–µ—Ä–Ω–∏ –µ–≥–æ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.\n"
        "3. –í –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {template_items}\n\n"
        "–°–ø–∏—Å–æ–∫ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:\n{items_to_correct}"
    )
    prompt = ChatPromptTemplate.from_messages([("system", prompt_text), ("user", "–ò—Å–ø—Ä–∞–≤—å —ç—Ç–æ—Ç —Å–ø–∏—Å–æ–∫ –∏ –≤–µ—Ä–Ω–∏ JSON-–æ–±—ä–µ–∫—Ç.")])
    chain = prompt | _llm | parser
    try:
        items_json_string = json.dumps(source_items_to_correct, ensure_ascii=False)
        correction_map = chain.invoke({"template_items": "\n".join(IFRS_FLAT_LIST), "items_to_correct": items_json_string})
        corrected_raw_data = copy.deepcopy(raw_data)
        for item in corrected_raw_data:
            original_item = item['source_item']
            if original_item in correction_map: item['source_item'] = correction_map[original_item]
        return corrected_raw_data
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π: {e}"); return raw_data

# --- –û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –°–¢–ê–ù–î–ê–†–¢–ò–ó–ê–¶–ò–ò (–ë–ï–ó –†–ê–°–ß–ï–¢–ê) ---
@st.cache_data
def standardize_data_with_aggregation(_llm, corrected_data: list, report_type: str) -> dict:
    """
    –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º —à–∞–±–ª–æ–Ω–æ–º –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –∏—Ö,
    –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ–ª–Ω—É—é —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç.
    """
    # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–ª–µ–≤–æ–π –ø–ª–æ—Å–∫–∏–π —à–∞–±–ª–æ–Ω –¥–ª—è –≤—ã–≤–æ–¥–∞
    template = REPORT_TEMPLATES.get(report_type, {})
    template_items_str = "\n".join([f"- {en_name} ({details['ru']})" for en_name, details in template.items()])
    
    # –ü–æ–ª–Ω–∞—è —Ç–∞–∫—Å–æ–Ω–æ–º–∏—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    full_taxonomy_str = json.dumps(IFRS_TAXONOMY, ensure_ascii=False, indent=2)[:5000]

    parser = JsonOutputParser()
    prompt_text = (
        "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ú–°–§–û. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å —Å—ã—Ä—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –¶–ï–õ–ï–í–´–ú —à–∞–±–ª–æ–Ω–æ–º –∏ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è. "
        "–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON-–æ–±—ä–µ–∫—Ç–æ–º —Å –î–í–£–ú–Ø –∫–ª—é—á–∞–º–∏: `standardized_data` –∏ `unmapped_items`.\n\n"
        "–¶–ï–õ–ï–í–û–ô –®–ê–ë–õ–û–ù (–∏—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —ç—Ç–∏ —Å—Ç–∞—Ç—å–∏ –≤ `standardized_data`):\n{target_template}\n\n"
        "–ü–û–õ–ù–ê–Ø –¢–ê–ö–°–û–ù–û–ú–ò–Ø –ú–°–§–û –î–õ–Ø –ö–û–ù–¢–ï–ö–°–¢–ê (–ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å, —á—Ç–æ –∫—É–¥–∞ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è):\n{full_taxonomy}\n\n"
        "–ü–†–ê–í–ò–õ–ê:\n"
        "1. –î–ª—è –ö–ê–ñ–î–û–ô —Å—Ç–∞—Ç—å–∏ –∏–∑ –¶–ï–õ–ï–í–û–ì–û –®–ê–ë–õ–û–ù–ê –Ω–∞–π–¥–∏ –≤ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤—Å–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –µ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã.\n"
        "2. –ê–ì–†–ï–ì–ò–†–£–ô (—Å—É–º–º–∏—Ä—É–π) –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞.\n"
        "   –ü—Ä–∏–º–µ—Ä: –µ—Å–ª–∏ –≤ —Ü–µ–ª–µ–≤–æ–º —à–∞–±–ª–æ–Ω–µ –µ—Å—Ç—å 'Assets', –∞ –≤ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –µ—Å—Ç—å 'NoncurrentAssets' –∏ 'CurrentAssets', "
        "   —Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º –¥–ª—è 'Assets' –±—É–¥–µ—Ç —Å—É–º–º–∞ 'NoncurrentAssets' + 'CurrentAssets'.\n"
        "3. –í `standardized_data` –≤–∫–ª—é—á–∞–π –¢–û–õ–¨–ö–û —Å—Ç–∞—Ç—å–∏ –∏–∑ –¶–ï–õ–ï–í–û–ì–û –®–ê–ë–õ–û–ù–ê.\n"
        "4. –í `unmapped_items` –ø–æ–º–µ—Å—Ç–∏ —Å—ã—Ä—ã–µ —Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –Ω–∏ –≤ –æ–¥–Ω—É —Ü–µ–ª–µ–≤—É—é —Å—Ç–∞—Ç—å—é.\n"
        "5. –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è `standardized_data` –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n"
        "   [ {{\"line_item\": \"–Ω–∞–∑–≤–∞–Ω–∏–µ_–∏–∑_–¶–ï–õ–ï–í–û–ì–û_—à–∞–±–ª–æ–Ω–∞\", \"unit\": \"–µ–¥_–∏–∑–º\", \"values\": [ {{\"period\": \"2024\", \"value\": –ê–ì–†–ï–ì–ò–†–û–í–ê–ù–ù–û–ï_—á–∏—Å–ª–æ}}] }} ]\n\n"
        "–°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ:\n{raw_data}"
    )
    prompt = ChatPromptTemplate.from_messages([
        ("system", prompt_text),
        ("user", "–í—ã–ø–æ–ª–Ω–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—é —Å—Ç—Ä–æ–≥–æ –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º –∏ –≤–µ—Ä–Ω–∏ JSON.")
    ])
    chain = prompt | _llm | parser
    
    try:
        raw_data_str = json.dumps(corrected_data, ensure_ascii=False, indent=2)[:100000]
        result = chain.invoke({
            "target_template": template_items_str,
            "full_taxonomy": full_taxonomy_str,
            "raw_data": raw_data_str
        })
        
        if "standardized_data" not in result: result["standardized_data"] = []
        if "unmapped_items" not in result: result["unmapped_items"] = []
            
        return result
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏: {e}")
        return {"standardized_data": [], "unmapped_items": corrected_data}


# --- –£–ü–†–û–©–ï–ù–ù–´–ï –§–£–ù–ö–¶–ò–ò –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø ---

def flatten_data_for_display(data: list, report_type: str) -> list:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –ø–ª–æ—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è DataFrame"""
    flat_list = []
    translation_dict = get_translation_map(report_type)
    
    for item in data:
        english_name = item.get("line_item")
        item_info = translation_dict.get(english_name, {})
        russian_name = item_info.get("ru", english_name)
        code = item_info.get("code", "N/A")
        
        for period_data in item.get("values", []):
            flat_list.append({
                "–ö–æ–¥ —Å—Ç–∞—Ç—å–∏": code,
                "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è": russian_name,
                "–ï–¥. –∏–∑–º.": item.get("unit", "N/A"),
                "–ü–µ—Ä–∏–æ–¥": period_data.get("period"),
                "–ò—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ": period_data.get("value")
            })
    return flat_list

def display_raw_data(raw_data):
    """
    –°–æ–∑–¥–∞–µ—Ç DataFrame –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—ã—Ä—ã—Ö –∏–ª–∏ –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    –£–°–¢–û–ô–ß–ò–í–ê–Ø –í–ï–†–°–ò–Ø: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º.
    """
    if not raw_data: 
        return pd.DataFrame()
        
    rows = []
    for item in raw_data:
        # --- –î–û–ë–ê–í–õ–ï–ù–ê –ü–†–û–í–ï–†–ö–ê ---
        # –ï—Å–ª–∏ item –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É
        if not isinstance(item, dict):
            continue
        # --------------------------
            
        for val in item.get("values", []):
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ 'val' —Ç–æ–∂–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º
            if not isinstance(val, dict):
                continue

            rows.append({
                "–ò—Å—Ö–æ–¥–Ω–∞—è —Å—Ç–∞—Ç—å—è": item.get('source_item', 'N/A'),
                "–ü–µ—Ä–∏–æ–¥": val.get("period", "N/A"),
                "–ó–Ω–∞—á–µ–Ω–∏–µ": val.get("value", "N/A"),
                "–ï–¥. –∏–∑–º.": item.get("unit", "")
            })
    return pd.DataFrame(rows)

def to_excel_bytes(df):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –æ–¥–∏–Ω DataFrame –≤ –±–∞–π—Ç—ã Excel —Ñ–∞–π–ª–∞"""
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='–û—Ç—á–µ—Ç')
    return output.getvalue()

def transform_to_wide_format(long_df):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –≤ —à–∏—Ä–æ–∫–∏–π"""
    if long_df.empty: return pd.DataFrame()
    wide_df = long_df.pivot_table(
        index=['–ö–æ–¥ —Å—Ç–∞—Ç—å–∏', '–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è', '–ï–¥. –∏–∑–º.'],
        columns='–ü–µ—Ä–∏–æ–¥',
        values='–ò—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ',
        aggfunc='first'
    ).reset_index()
    wide_df.columns.name = None
    period_cols = sorted([col for col in wide_df.columns if col not in ['–ö–æ–¥ —Å—Ç–∞—Ç—å–∏', '–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è', '–ï–¥. –∏–∑–º.']], reverse=True)
    return wide_df[['–ö–æ–¥ —Å—Ç–∞—Ç—å–∏', '–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è', '–ï–¥. –∏–∑–º.'] + period_cols]


# --- –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ò–ù–¢–ï–†–§–ï–ô–° –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø ---
st.title("üìä –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏")

st.sidebar.header("–ó–∞–≥—Ä—É–∑–∫–∞ –§–∞–π–ª–æ–≤")
uploaded_files = st.sidebar.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å–∫–∞–Ω—ã –æ—Ç—á–µ—Ç–∞", type=["pdf", "png", "jpg", "jpeg"], accept_multiple_files=True
)

if uploaded_files:
    file_names = [f.name for f in uploaded_files]
    # –°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–æ–≤
    if "processed_data" not in st.session_state or st.session_state.get("file_names") != file_names:
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.session_state.file_names = file_names

    # --- –ü–û–®–ê–ì–û–í–û–ï –í–´–ü–û–õ–ù–ï–ù–ò–ï ---

    if "all_text" not in st.session_state:
        with st.spinner("–®–∞–≥ 1/5: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞..."):
            all_text = ""
            for uploaded_file in uploaded_files:
                file_text = extract_text_from_file(uploaded_file.getvalue(), uploaded_file.name)
                if file_text: all_text += f"\n\n--- –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: {uploaded_file.name} ---\n\n{file_text}"
            st.session_state.all_text = all_text.strip()
    
    if "report_type" not in st.session_state:
        with st.spinner("–®–∞–≥ 2/5: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –æ—Ç—á–µ—Ç–∞..."):
            st.session_state.report_type = classify_report(llm, st.session_state.all_text)
    report_type = st.session_state.report_type
    if report_type == "Unknown": st.error("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –æ—Ç—á–µ—Ç–∞."); st.stop()
    st.success(f"‚úÖ –û—Ç—á–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ **{report_type}**.")

    if "raw_data" not in st.session_state:
        with st.spinner("–®–∞–≥ 3/5: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö..."):
            st.session_state.raw_data = extract_raw_financial_data(llm, st.session_state.all_text)

    if "corrected_data" not in st.session_state:
        with st.spinner("–®–∞–≥ 4/5: –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π..."):
            st.session_state.corrected_data = correct_source_item_names(llm, st.session_state.raw_data)

    if "processed_data" not in st.session_state:
        with st.spinner("–®–∞–≥ 5/5: –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö..."):
            response_dict = standardize_data_with_aggregation(llm, st.session_state.corrected_data, report_type)
            st.session_state.processed_data = response_dict.get("standardized_data", [])
            st.session_state.unmapped_items = response_dict.get("unmapped_items", [])

    # --- –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ---
    st.success("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    st.header("–ò—Ç–æ–≥–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç")

    processed_data = st.session_state.processed_data
    if processed_data:
        flat_data = flatten_data_for_display(processed_data, report_type)
        long_df = pd.DataFrame(flat_data)
        wide_df = transform_to_wide_format(long_df)
        
        st.dataframe(wide_df, use_container_width=True, hide_index=True)
        
        excel_bytes = to_excel_bytes(wide_df)
        st.download_button(
            "üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á–µ—Ç –≤ Excel", 
            excel_bytes, 
            f"standard_report_{report_type.replace(' ', '_')}.xlsx",
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    else:
        st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")

    unmapped_items = st.session_state.unmapped_items
    if unmapped_items:
        with st.expander("‚ö†Ô∏è –ù–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏"):
            unmapped_df = display_raw_data(unmapped_items)
            st.dataframe(unmapped_df, use_container_width=True, hide_index=True)

    with st.expander("üîç –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)"):
        st.subheader("–°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ (–≤—Ö–æ–¥ –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏)")
        st.json(st.session_state.corrected_data)
        st.subheader("–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–≤—ã—Ö–æ–¥)")
        st.json(st.session_state.processed_data)

else:
    st.info("üëà –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å.")
