import os
import sys
import streamlit as st

# –û—Å—Ç–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
import fitz
import pytesseract
from pdf2image import convert_from_bytes
from PIL import Image
import io
import pandas as pd
import json
import tempfile

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π –¥–ª—è Tesseract –≤ Streamlit Cloud
if os.path.exists('/app'):
    os.environ['TESSDATA_PREFIX'] = '/usr/share/tesseract-ocr/4.00/tessdata'
    pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'

## –ò–ó–ú–ï–ù–ï–ù–ò–ï: –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é get_translation_map
from templates import REPORT_TEMPLATES, get_report_template_as_string, get_translation_map

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
st.set_page_config(layout="wide", page_title="–£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞")

# --- –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø LLM ---
try:
    PROVIDER_API_KEY = os.getenv("PROVIDER_API_KEY")
    if not PROVIDER_API_KEY:
        st.error("API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.")
        st.stop()
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ API –∫–ª—é—á–∞: {e}")
    st.stop()

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser

llm = ChatOpenAI(
    model_name="deepseek/deepseek-r1-0528",
    openai_api_key=PROVIDER_API_KEY,
    openai_api_base="https://api.novita.ai/v3/openai",
    temperature=0.1
)

# --- –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø –§–£–ù–ö–¶–ò–ô ---

@st.cache_data
def extract_text_from_file(file_bytes, filename):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π OCR."""
    try:
        ext = filename.split(".")[-1].lower()
        text = ""
        
        if ext == "pdf":
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF
            with fitz.open(stream=file_bytes, filetype="pdf") as doc:
                for page in doc:
                    text += page.get_text()
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ–∫—Å—è, –ø—Ä–æ–±—É–µ–º OCR
            if not text.strip():
                st.warning("PDF –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É—é OCR...")
                images = convert_from_bytes(file_bytes, dpi=300)
                for image in images:
                    text += pytesseract.image_to_string(image, lang='rus+eng')
                    
        elif ext in ["png", "jpg", "jpeg"]:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            image = Image.open(io.BytesIO(file_bytes))
            # –£–ª—É—á—à–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è OCR
            if image.mode != 'RGB':
                image = image.convert('RGB')
            text = pytesseract.image_to_string(image, lang='rus+eng')
        else:
            st.error(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {ext}")
            return None
            
        return text.strip()
    
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
        return None

@st.cache_data
def classify_report(_llm, text: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –æ—Ç—á–µ—Ç–∞ (–±–∞–ª–∞–Ω—Å, –û–ü–£ –∏ —Ç.–¥.) —Å –ø–æ–º–æ—â—å—é LLM."""
    # –ó–∞–ø—Ä–æ—Å –∫ LLM
    prompt = ChatPromptTemplate.from_template(
        "–¢–µ–∫—Å—Ç —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞: {text}\n\n"
        "–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞. –í—ã–±–µ—Ä–∏ –æ–¥–∏–Ω –∏–∑: {report_types}.\n"
        "–û—Ç–≤–µ—Ç –≤—ã–¥–∞–π —Ç–æ–ª—å–∫–æ –∫–∞–∫ —Å—Ç—Ä–æ–∫—É, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."
    )
    chain = prompt | _llm | StrOutputParser()
    report_types_str = ", ".join(REPORT_TEMPLATES.keys())
    return chain.invoke({"text": text, "report_types": report_types_str})

@st.cache_data
def extract_data_with_template(_llm, text: str, report_type: str) -> list:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ—Ç—á–µ—Ç–∞ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É —à–∞–±–ª–æ–Ω—É."""
    # –ü–æ–ª—É—á–∞–µ–º —à–∞–±–ª–æ–Ω –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ –æ—Ç—á–µ—Ç–∞
    template = get_report_template_as_string(report_type)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä
    parser = JsonOutputParser()
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç
    prompt = ChatPromptTemplate.from_template(
        template + "\n\n–ò–∑–≤–ª–µ–∫–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞:\n{text}"
    )
    
    chain = prompt | _llm | parser
    return chain.invoke({"text": text})

def to_excel_bytes(df):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç DataFrame –≤ –±–∞–π—Ç—ã Excel —Ñ–∞–π–ª–∞."""
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='Report')
    return output.getvalue()

def enrich_data_with_russian_names(data: list, report_type: str) -> list:
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Ä—É—Å—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–∞—Ç–µ–π –≤ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ."""
    translation_map = get_translation_map(report_type)
    enriched_data = []
    
    for item in data:
        english_name = item.get("line_item")
        if not english_name:
            continue
            
        russian_name = translation_map.get(english_name, "–°—Ç–∞—Ç—å—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —à–∞–±–ª–æ–Ω–µ")
        
        enriched_data.append({
            "–°—Ç–∞—Ç—å—è (RU)": russian_name,
            "Line Item (EN)": english_name,
            "value": item.get("value"),
            "year": item.get("year"),
            "unit": item.get("unit")
        })
    return enriched_data

# --- –ò–ù–¢–ï–†–§–ï–ô–° –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø ---
st.title("ü§ñ –£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞")

st.sidebar.header("–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤")
uploaded_files = st.sidebar.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å–∫–∞–Ω—ã –æ—Ç—á–µ—Ç–∞ (–æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤)",
    type=["pdf", "png", "jpg", "jpeg"],
    accept_multiple_files=True
)

if uploaded_files:
    all_text = ""
    
    for uploaded_file in uploaded_files:
        file_bytes = uploaded_file.getvalue()
        
        with st.spinner(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ {uploaded_file.name}..."):
            extracted_text = extract_text_from_file(file_bytes, uploaded_file.name)
        
        if extracted_text:
            st.success(f"‚úÖ –¢–µ–∫—Å—Ç –∏–∑ {uploaded_file.name} —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω!")
            all_text += f"\n\n--- –§–ê–ô–õ: {uploaded_file.name} ---\n\n{extracted_text}"
        else:
            st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ {uploaded_file.name}.")
    
    if not all_text.strip():
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –Ω–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.")
        st.stop()
    
    # –ü–æ–∫–∞–∂–µ–º –æ–±—â–∏–π –æ–±—ä–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    st.info(f"–û–±—â–∏–π –æ–±—ä–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {len(all_text)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"):
        st.text(all_text[:5000] + ("..." if len(all_text) > 5000 else ""))
    
    with st.spinner("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –æ—Ç—á–µ—Ç–∞..."):
        report_type = classify_report(llm, all_text)
    
    if report_type not in REPORT_TEMPLATES:
        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –æ—Ç—á–µ—Ç–∞. LLM –≤–µ—Ä–Ω—É–ª: '{report_type}'")
    else:
        st.success(f"‚úÖ –û—Ç—á–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ **{report_type}**.")
        
        with st.spinner(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ —à–∞–±–ª–æ–Ω—É..."):
            structured_data = extract_data_with_template(llm, all_text, report_type)
        
        if not structured_data:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")
        else:
            st.success("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã!")

            final_data = enrich_data_with_russian_names(structured_data, report_type)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            df = pd.DataFrame([item for item in final_data if item.get('value') is not None])
            
            st.header("–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
            if not df.empty:
                df = df[["–°—Ç–∞—Ç—å—è (RU)", "Line Item (EN)", "value", "year", "unit"]]
                st.dataframe(df, use_container_width=True)
                
                # –ö–Ω–æ–ø–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                excel_bytes = to_excel_bytes(df)
                st.download_button(
                    label="üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á–µ—Ç –≤ Excel",
                    data=excel_bytes,
                    file_name="standard_report.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
            else:
                st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
            
            with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω—ã–π JSON –æ—Ç LLM (–¥–æ –æ–±–æ–≥–∞—â–µ–Ω–∏—è)"):
                st.json(structured_data)
else:
    st.info("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å.")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –≤ session_state
st.session_state['extract_text_from_file_func'] = extract_text_from_file
st.session_state['classify_report_func'] = classify_report
st.session_state['extract_data_with_template_func'] = extract_data_with_template
